inspect(corpus_noEmailAddress[[1]])
removeRegex <- function(x) {
gsub(pattern = "subject:", replacement = "", x)
}
corpus_noEmailAddress <- tm_map(corpus_lowercase, content_transformer(removeRegex))
inspect(corpus_noEmailAddress[[1]])
corpus_noEmailAddress <- tm_map(corpus_noEmailAddress, stripWhitespace())
corpus_noEmailAddress <- tm_map(corpus_noEmailAddress, stripWhitespace = T)
corpus_noEmailAddress <- stripWhitespace(corpus_noEmailAddress)
corpus_noEmailAddress <- strsplit(corpus_noEmailAddress,  " ")
corpus_noEmailAddress <- tm_map(corpus_lowercase, content_transformer(removeRegex))
inspect(corpus_noEmailAddress[[1]])
inspect(corpus_noEmailAddress[[4]])
removeRegex <- function(x) {
gsub(pattern = "\\w*[[:blank:]][[:punct:]][[:blank:]]\\w*[[:blank:]][[:punct:]][[:blank:]]com", replacement = "", x)
}
corpus_noEmailAddress <- tm_map(corpus_lowercase, content_transformer(removeRegex))
inspect(corpus_noEmailAddress[[4]])
clear
clean
library(tm)
library(wordcloud)
library(SnowballC)
library(RWeka)
library(ggplot2)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
emails <- read.csv('emails.csv')
head(emails$text)
length(emails$text)
removeRegex <- function(x) {
gsub(pattern = "\\w*[[:blank:]][[:punct:]][[:blank:]]\\w*[[:blank:]][[:punct:]][[:blank:]]com", replacement = "", x)
}
corpus <- VCorpus(VectorSource(emails$text))
inspect(corpus[[1]])
inspect(corpus[[4]])
corpus_lowercase <- tm_map(corpus, content_transformer(tolower))
inspect(corpus_lowercase[[1]])
corpus_noEmailAddress <- tm_map(corpus_lowercase, content_transformer(removeRegex))
inspect(corpus_noEmailAddress[[4]])
length(corpus_lowercase)
length(corpus_noEmailAddress)
tdm = TermDocumentMatrix(corpus_noEmailAddress, control=list(removePunctuation = T, removeNumbers = T, stopwords = c(stopwords(), "subject", "subject re"), stripWhitespace= T))
tdm.small <- removeSparseTerms(tdm, sparse = 0.9)
tdm.small
freq = rowSums(as.matrix(tdm.small))
head(freq,10)
pal = brewer.pal(8, "Blues")
pal = pal[-(1:3)]
set.seed(1234)
freq = sort(rowSums(as.matrix(tdm.small)), decreasing = T)
word.cloud = wordcloud(words=names(freq), freq=freq, min.freq=500,
random.order=F, colors=pal)
inspect(tdm)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T ))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freq), freq=freq)
head(freq.df, 20)
head(freqBigram.df, 20)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
removeWords(tdm.bigram, "")
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, removeWords("")))
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T))
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = ""))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = " "))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = "ect"))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = " ect"))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
trimws(tdm.bigram)
trimws(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
tdm.bigram.small.trimmed <- trimws(tdm.bigram.small)
tdm.bigram.small <- trimws(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
inspect(tdm.bigram.small)
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
tdm.bigram.small <- tm_map(tdm.bigram.small, trimws())
tdm.bigram.small <- tm_map(tdm.bigram.small, trimws)
tdm.bigram.small <- tm_map(tdm.bigram.small, stripWhitespace)
corpus_noWhiteSpace <- tm_map(corpus_noEmailAddress, stripWhitespace)
tdm.bigram <- TermDocumentMatrix(corpus_noWhiteSpace,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
removeRegex <- function(x) {
gsub(pattern = "\\w*[[:blank:]][[:punct:]][[:blank:]]\\w*[[:blank:]][[:punct:]][[:blank:]]com[:blank:]]", replacement = "", x)
}
corpus_noEmailAddress <- tm_map(corpus_lowercase, content_transformer(removeRegex))
inspect(corpus_noEmailAddress[[4]])
gsub(pattern = "\\w*[[:blank:]][[:punct:]][[:blank:]]\\w*[[:blank:]][[:punct:]][[:blank:]]com[[:blank:]]", replacement = "", x)
removeRegex <- function(x) {
gsub(pattern = "\\w*[[:blank:]][[:punct:]][[:blank:]]\\w*[[:blank:]][[:punct:]][[:blank:]]com[[:blank:]]", replacement = "", x)
}
corpus_noEmailAddress <- tm_map(corpus_lowercase, content_transformer(removeRegex))
inspect(corpus_noEmailAddress[[4]])
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(" ect", "subject re", "ect ", "hou", "vince", "hou ", " hou")))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou")))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron")))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski")))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
tdm.bigram <- TermDocumentMatrix(corpus_noEmailAddress,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
removeEmailAddress <- function(x) {
gsub(pattern = "\\w*[[:blank:]][[:punct:]][[:blank:]]\\w*[[:blank:]][[:punct:]][[:blank:]]com[[:blank:]]", replacement = "", x)
}
corpus_noEmailAddress <- tm_map(corpus_lowercase, content_transformer(removeEmailAddress))
removeNumeration <- function(x) {
gsub(pattern = "[[:digit:]][[:blank:]]th", replacement = "", x)
}
corpus_noNumeration <- tm_map(corpus_noEmailAddress, content_transformer(removeNumeration))
tdm.bigram <- TermDocumentMatrix(corpus_noNumeration,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
tdm.bigram <- TermDocumentMatrix(corpus_noNumeration,
control = list (tokenize = BigramTokenizer, stripWhitespace = T , stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
tdm.bigram <- TermDocumentMatrix(corpus_noNumeration,
control = list (tokenize = BigramTokenizer, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
tdm.bigram <- TermDocumentMatrix(corpus_noNumeration,
control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
##################### BIGRAM #####################
corpus.ngrams = tm_map(corpus_noNumeration,removeWords,c(myStopwords,"re"))
corpus.ngrams = tm_map(corpus.ngrams,removePunctuation)
##################### BIGRAM #####################
corpus.ngrams = tm_map(corpus_noNumeration,removeWords,c(stopwords(),"re"))
corpus.ngrams = tm_map(corpus.ngrams,removePunctuation)
corpus.ngrams = tm_map(corpus.ngrams,removeNumbers)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
# tdm.bigram <- TermDocumentMatrix(corpus_noNumeration,
#                                 control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram <- TermDocumentMatrix(corpus.ngrams,
control = list (tokenize = BigramTokenizer))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
##################### BIGRAM #####################
corpus.ngrams = tm_map(corpus_noNumeration,removeWords,c(stopwords(),"re", "ect", "hou", "e", "mail", "kaminski", "hou", "cc", "subject", "j"))
corpus.ngrams = tm_map(corpus.ngrams,removePunctuation)
corpus.ngrams = tm_map(corpus.ngrams,removeNumbers)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
# tdm.bigram <- TermDocumentMatrix(corpus_noNumeration,
#                                 control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram <- TermDocumentMatrix(corpus.ngrams,
control = list (tokenize = BigramTokenizer))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
freqBigram = sort(rowSums(as.matrix(tdm.bigram)),decreasing = TRUE)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
ggplot(head(freq.df,15), aes(reorder(word,freq), freq)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
##################### BIGRAM #####################
corpus.ngrams = tm_map(corpus_noNumeration,removeWords,c(stopwords(),"re", "ect", "hou", "e", "mail", "kaminski", "hou", "cc", "subject"))
corpus.ngrams = tm_map(corpus.ngrams,removePunctuation)
corpus.ngrams = tm_map(corpus.ngrams,removeNumbers)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
# tdm.bigram <- TermDocumentMatrix(corpus_noNumeration,
#                                 control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram <- TermDocumentMatrix(corpus.ngrams,
control = list (tokenize = BigramTokenizer))
freq = sort(rowSums(as.matrix(tdm.bigram)),decreasing = TRUE)
freq.df = data.frame(word=names(freq), freq=freq)
tdm.bigram.small <- removeSparseTerms(tdm, sparse = 0.9)
tdm.bigram.small
freq = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freq.df = data.frame(word=names(freq), freq=freq)
head(freq.df, 20)
wordcloud(freq.df$word,freq.df$freq,max.words=100,random.order = F, colors=pal)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
##################### BIGRAM #####################
corpus.ngrams = tm_map(corpus_noNumeration,removeWords,c(stopwords(),"re", "ect", "hou", "e", "mail", "kaminski", "hou", "cc", "subject"))
corpus.ngrams = tm_map(corpus.ngrams,removePunctuation)
corpus.ngrams = tm_map(corpus.ngrams,removeNumbers)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
# tdm.bigram <- TermDocumentMatrix(corpus_noNumeration,
#                                 control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram <- TermDocumentMatrix(corpus.ngrams,
control = list (tokenize = BigramTokenizer))
tdm.bigram.small <- removeSparseTerms(tdm, sparse = 0.9)
tdm.bigram.small
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
library(tm)
library(wordcloud)
library(SnowballC)
library(RWeka)
library(ggplot2)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
emails <- read.csv('emails.csv')
head(emails$text)
length(emails$text)
removeEmailAddress <- function(x) {
gsub(pattern = "\\w*[[:blank:]][[:punct:]][[:blank:]]\\w*[[:blank:]][[:punct:]][[:blank:]]com[[:blank:]]", replacement = "", x)
}
removeNumeration <- function(x) {
gsub(pattern = "[[:digit:]][[:blank:]]th", replacement = "", x)
}
corpus <- VCorpus(VectorSource(emails$text))
inspect(corpus[[4]])
corpus_lowercase <- tm_map(corpus, content_transformer(tolower))
inspect(corpus_lowercase[[1]])
corpus_noEmailAddress <- tm_map(corpus_lowercase, content_transformer(removeEmailAddress))
inspect(corpus_noEmailAddress[[4]])
corpus_noNumeration <- tm_map(corpus_noEmailAddress, content_transformer(removeNumeration))
tdm = TermDocumentMatrix(corpus_noEmailAddress, control=list(removePunctuation = T, removeNumbers = T, stopwords = c(stopwords(), "subject", "subject re"), stripWhitespace= T))
##################### BIGRAM #####################
corpus.ngrams = tm_map(corpus_noNumeration,removeWords,c(stopwords(),"re", "ect", "hou", "e", "mail", "kaminski", "hou", "cc", "subject"))
corpus.ngrams = tm_map(corpus.ngrams,removePunctuation)
corpus.ngrams = tm_map(corpus.ngrams,removeNumbers)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
# tdm.bigram <- TermDocumentMatrix(corpus_noNumeration,
#                                 control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram <- TermDocumentMatrix(corpus.ngrams,
control = list (tokenize = BigramTokenizer))
tdm.bigram.small <- removeSparseTerms(tdm, sparse = 0.9)
tdm.bigram.small
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
##################### BIGRAM #####################
corpus.ngrams = tm_map(corpus_noNumeration,removeWords,c(stopwords(),"re", "ect", "hou", "e", "mail", "kaminski", "hou", "cc", "subject"))
corpus.ngrams = tm_map(corpus.ngrams,removePunctuation)
corpus.ngrams = tm_map(corpus.ngrams,removeNumbers)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
# tdm.bigram <- TermDocumentMatrix(corpus_noNumeration,
#                                 control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram <- TermDocumentMatrix(corpus.ngrams,
control = list (tokenize = BigramTokenizer))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.9)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
pal = brewer.pal(8, "Blues")
pal = pal[-(1:3)]
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.95)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
ggplot(head(freq.df,15), aes(reorder(word,freq), freq)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
ggplot(head(freqBigram.df,15), aes(reorder(word,freqBigram), freqBigram)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
ggplot(head(freqBigram.df,15), aes(reorder(word,freqBigram), freq)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
ggplot(head(freqBigram.df,15), aes(reorder(word,freqBigram), freqBigram)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
ggplot(head(freqBigram.df,15), aes(reorder(word,freq), freqBigram)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
Bigram
ggplot(head(freqBigram.df,15), aes(reorder(word,freqBigram), freqBigram)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
ggplot(head(freqBigram.df,15), aes(reorder(word,freq), freqBigram)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
ggplot(head(freqBigram.df,15), aes(reorder(word,freqBigram), freq)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
ggplot(head(freqBigram.df,15), aes(reorder(word,freqBigram), freqBigram)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
library(tm)
library(wordcloud)
library(SnowballC)
library(RWeka)
library(ggplot2)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
emails <- read.csv('emails.csv')
head(emails$text)
length(emails$text)
removeEmailAddress <- function(x) {
gsub(pattern = "\\w*[[:blank:]][[:punct:]][[:blank:]]\\w*[[:blank:]][[:punct:]][[:blank:]]com[[:blank:]]", replacement = "", x)
}
removeNumeration <- function(x) {
gsub(pattern = "[[:digit:]][[:blank:]]th", replacement = "", x)
}
corpus <- VCorpus(VectorSource(emails$text))
inspect(corpus[[4]])
corpus_lowercase <- tm_map(corpus, content_transformer(tolower))
inspect(corpus_lowercase[[1]])
corpus_noEmailAddress <- tm_map(corpus_lowercase, content_transformer(removeEmailAddress))
inspect(corpus_noEmailAddress[[4]])
corpus_noNumeration <- tm_map(corpus_noEmailAddress, content_transformer(removeNumeration))
tdm = TermDocumentMatrix(corpus_noEmailAddress, control=list(removePunctuation = T, removeNumbers = T, stopwords = c(stopwords(), "subject", "subject re"), stripWhitespace= T))
tdm.small <- removeSparseTerms(tdm, sparse = 0.9)
tdm.small
freq = rowSums(as.matrix(tdm.small))
head(freq,10)
pal = brewer.pal(8, "Blues")
pal = pal[-(1:3)]
set.seed(1234)
freq = sort(rowSums(as.matrix(tdm.small)), decreasing = T)
word.cloud = wordcloud(words=names(freq), freq=freq, min.freq=500,
random.order=F, colors=pal)
tdm = TermDocumentMatrix(corpus_noEmailAddress, control=list(removePunctuation = T, removeNumbers = T, stopwords = c(stopwords(), "subject", "subject re", "vince", "hou", "ect", "kaminski"), stripWhitespace= T))
tdm.small <- removeSparseTerms(tdm, sparse = 0.9)
tdm.small
freq = rowSums(as.matrix(tdm.small))
head(freq,10)
pal = brewer.pal(8, "Blues")
pal = pal[-(1:3)]
set.seed(1234)
freq = sort(rowSums(as.matrix(tdm.small)), decreasing = T)
word.cloud = wordcloud(words=names(freq), freq=freq, min.freq=500,
random.order=F, colors=pal)
inspect(tdm)
##################### BIGRAM #####################
corpus.ngrams = tm_map(corpus_noNumeration,removeWords,c(stopwords(),"re", "ect", "hou", "e", "mail", "kaminski", "hou", "cc", "subject"))
corpus.ngrams = tm_map(corpus.ngrams,removePunctuation)
corpus.ngrams = tm_map(corpus.ngrams,removeNumbers)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
# tdm.bigram <- TermDocumentMatrix(corpus_noNumeration,
#                                 control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram <- TermDocumentMatrix(corpus.ngrams,
control = list (tokenize = BigramTokenizer))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.95)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.97)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
ggplot(head(freqBigram.df,15), aes(reorder(word,freqBigram), freqBigram)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
##################### BIGRAM #####################
corpus.ngrams = tm_map(corpus_noNumeration,removeWords,c(stopwords(),"re", "ect", "hou", "e", "mail", "kaminski", "hou", "cc", "subject", "vince", "j"))
corpus.ngrams = tm_map(corpus.ngrams,removePunctuation)
corpus.ngrams = tm_map(corpus.ngrams,removeNumbers)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
# tdm.bigram <- TermDocumentMatrix(corpus_noNumeration,
#                                 control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram <- TermDocumentMatrix(corpus.ngrams,
control = list (tokenize = BigramTokenizer))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.97)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
ggplot(head(freqBigram.df,15), aes(reorder(word,freqBigram), freqBigram)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Bigrams") + ylab("Frequency") +
ggtitle("Most frequent bigrams")
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
##################### BIGRAM #####################
corpus.ngrams = tm_map(corpus_noNumeration,removeWords,c(stopwords(),"re", "ect", "hou", "e", "mail", "kaminski", "hou", "cc", "subject", "vince", "j", "enron"))
corpus.ngrams = tm_map(corpus.ngrams,removePunctuation)
corpus.ngrams = tm_map(corpus.ngrams,removeNumbers)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
# tdm.bigram <- TermDocumentMatrix(corpus_noNumeration,
#                                 control = list (tokenize = BigramTokenizer, removeNumbers = T, stripWhitespace = T , removePunctuation = T, stopwords = c(stopwords(), " ect", "subject re", "ect ", "hou", "vince", "hou ", " hou", "cc subject", " am", "on ", "enron ", " enron", " pm", "j kaminski", "kaminski ", "http", "ect cc", " to")))
tdm.bigram <- TermDocumentMatrix(corpus.ngrams,
control = list (tokenize = BigramTokenizer))
tdm.bigram
tdm.bigram.small <- removeSparseTerms(tdm.bigram, 0.99)
inspect(tdm.bigram.small)
freqBigram = sort(rowSums(as.matrix(tdm.bigram.small)),decreasing = TRUE)
freqBigram.df = data.frame(word=names(freqBigram), freq=freqBigram)
head(freqBigram.df, 20)
wordcloud(freqBigram.df$word,freqBigram.df$freq,max.words=100,random.order = F, colors=pal)
